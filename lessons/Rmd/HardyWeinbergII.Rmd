---
title: 
- <center>More on Hardy Weinberg</center>
output:
  html_document:
    css: Chapter.css
    number_sections: yes
    toc: yes
    toc_depth: 5
---

```{r, echo=FALSE}
library(TeachingPopGen)
library(knitr)
```

### Exploring in More Depth

So at this point, we've seen how we can calculate allele frequencies, and we've also seen that in an ideal population, those frequencies do not change - the population does not evolve.  And it is worth it at this point to briefly summarize the properties of an ideal population:

1.  It is infinitely large
2.  Mating is random
3.  There is no mutation
4.  There is no migration
5.  There is no selection

Much of the rest of our time will be focused on understanding the consequences departures from one or more of these conditions - how do we detect them and what are their consequences?  Before that, however, we need to delve into the Hardy Weinberg Equilibrium a bit more deeply.

### Testing for departures from Hardy Weinberg

This is a subject we will be exploring in more depth in a later unit, but for starters, let's keep it simple.  Assume we can observe some genotypes:

Genotype| N
---|---
AA | 56
Aa | 78
aa | 22

We can calculate the frequencies of the two alleles as we did before:

```{r}
obs <-c(56,78,22)
N <-sum(obs)
p <-(2*obs[1]+obs[2])/(2*N)
q <- 1-p
p; q
```
And from those, we can calculate our expected genotype *numbers*
```{r}
exp <-N*c(p^2,2*p*q,q^2)
exp
```
So now we have observed and expected numbers; it would seem a logical step to apply the chi-square test to see if the difference between the two is significant. What we are doing is quantitating the departure from the expectations of an hypothesis for categorical data.  In our case, the categories are genotypes; the hypothesis is that the population is in Hardy-Weinberg equilibrium, and the data are the observed genotype *numbers*, which are compared to those expected based on HWE.

We've done the calculations above, so &chi;^2^ can be calculated easily by


```{r}
chi <-sum((obs-exp)^2/exp)
chi
```

But how many degrees of freedom?  One rule of thumb is that degrees of freedom are one less than the number of classes, suggesting that it might be two in this case.  However there is another wrinkle.  In this case we used the actual data to estimate a parameter used in determining our expected values - p.  That costs us another degree of freedom. Another way to think of it is that our genotype frequencies are actually derived from two allelic frequencies (p and q).  So in the case of testing Hardy Weinberg with data from a biallelic locus, we only have one degree of freedom.  

So what is the probability that we would observe this much deviation based on chance alone (in which case we would accept our hypothesis)?  One way to do that is simulation - generate a bunch of &chi;^2^ distributed random variables and see where the 5% cutoff falls.  In this case, we will do a *one tail test*, since the range of our statistic is from zero to infinity (with zero the value obtained if the observed data exactly match the expectation)

```{r}
set.seed(123)
ch <- rchisq(100000,1)
colhist(ch,tail=1, xr=c(0,20),xlab="chi-square",ylab="Number",main="Are the Data Consistent with HWE?")
abline(v=chi,col="blue")
quantile(ch,.95)
```

And we see that the data in fact fall within the range expected we observed as a result of chance.

We can also calculate the probability of this particular &chi;^2^ value as follows:

```{r}
pr <-1-pchisq(chi,1)
pr
```

And we see that the probability of observing this much or more deviation from Hardy-Weinberg expectations is 53%, far higher than our normal standard of 5% for rejecting the null hypothesis.  So we can conclude in this case that the hypothesis cannot be rejected.

#### The more conventional approach

When you've done &chi;^2^ testing in the past, you've probably used a table that lists degrees of freedom and cutoff values that have been determined analytically.  For example, for one degree of freedom, that value is 3.84; we see that it is very similar to the number we obtained from our simulation above.  A small version of this table is also shown below:

```{r}
df <-c(1:10)
chicrit.05 <-sapply(df, function(x) qchisq(.95,x))
chicrit.01 <-sapply(df,function (x) qchisq(.99,x))
kable(data.frame(df,p.05 = chicrit.05,p.01=chicrit.01))
```

#### A coding note

the hw() function in TeachingPopGen will accomplish much of what we have done so far.  Given some set of single locus, biallelic genotypes, it will return all the basic information.  For example

```{r}
dat.obs <-c(55,75,12) #observed numbers of AA, Aa, and aa
dat.hw <-hw(dat.obs)
```
Note that it will print the results to the console; it also returns a list, in this case to the variable dat.hw:

```{r}
str(dat.hw)
```


### Homozyogsity, Heterozygosity, and F


So by now we've looked at the chi-squared approach to testing whether observed genotype **numbers** are consistent with the prediction of Hardy-Weinberg.  There is, however, another way we can make this comparison - one that is less statistical, but which (as we shall see) is very relevant biologically.

#### A couple of definitions

We will use the following terms extensively in what follows:

* *Homozygosity* - the frequency of homozygotes (of any sort, e. g. AA and aa in the biallelic case) in a sample or population
* *Heterozygosity*  the total frequency of heterozygotes in the population

#### Expected Values

As is the case with any such measure, we need to have expectations.  These are pretty straightforward, especially:

1.  Expected homozygosity is simply ∑p<sub>i</sub>^2^, where the p's are the frequencies of the k alleles in the population
2.  Expected heterozygosity is easy for the biallelic case - it is simply 2p(1-p).  That could be extended to the case of multiple alleles, but remembering that every individual is either a homozygote or a heterozygote, a simpler calculation is

E(heterozygosity)=1-E(homozygosity) = 1- ∑p<sub>i</sub>^2^

#### Comparing some observations.

Now that we have our expectations, we can turn to some data.  These come from real data and are a staple of chapter-end problems in Genetics and Evolution texts.  The organisms are brown bears, the phenotypes are brown (dominant) vs. white(recessive) and the genotypes of a single base in the melanocortin receptor gene which is responsible for the difference are as follows:

Genotype | Phenotype | N
---|---|---
AA|Brown|42
AG|Brown|24
GG|White|21

So we can do some basic calculations

```{r}
genos <-c(AA=42,AG=24,GG=21)
genos
```
And we can do our basic Hardy-Weinberg calculations as we have before
```{r}
hw.genos <-hw(genos)
```
And we see that this is not an ideal population.  In fact, by inspection, we see that the observed number of heterozygotes, `r genos[2]`, is less than the expected one, `r hw.genos$Expected[2]`.  But we want to make this more quantitative, comparing the observed and expected homozygosity and heterozygosity.  But since these must add up to one, and for reasons that will become clear down the road, we'll focus on heterozygosity.  Furthermore, from here on out, we'll work with frequencies rather than numbers (although  the latter would work equally well).

So first, what are the observed frequencies?
```{r}
genos.f.obs <-genos/sum(genos)
genos.f.obs
```
The observed heterozygosity is thus `r round(genos.f.obs[2],3)`; to make the notation easier, we'll assign that a name
```{r}
Hobs <-genos.f.obs[2]
```
Now, what about the expected?  Remembering that that is 2pq, we calculate it as
```{r}
allele.freqs <-hw.genos$Allele_freqs
Hexp <-2*allele.freqs[1]*allele.freqs[2]
unname(Hexp)
```
Note that an alternate way of calculating this would simply be
```{r}
homo.exp <-(allele.freqs)^2
homo.exp <-sum(homo.exp)
unname(1-homo.exp)
```
Which gives us the same value.

### The F statistic

So what we want to do now is to come up with a numerical value to compare observed and expected heterozygosity.  Obviously, if they are equal (that is, we have a Hardy-Weinberg population), then the ratio will be 1.  Similarly, if there are no heterozygotes observed, then the ratio would be zero.  Thus, by subtracting the ratio from 1, we get a statistic that increases from zero to one, proportionate with increasing departure from HW.  Thus for our bear case here, we find that
```{r}
f.bears <-1-(Hobs/Hexp)
unname(f.bears)
```
This statistic has a variety of names - Wright's F Statistic and the Fixation Index are two common ones.  We won't say more about it at this point, however we will be using it extensively in the future.  But the key points to realize are

1.  If a population is in HWE, then Hobs=Hexp so F= 1-1 = 0
2.  If there are *fewer* heterozygotes observed than expected, then F will be positive (as in the bear case)
3.  If there are *more* heterozygotes observed than expected, then F is negative

So what is a significant value of F?  At this point, we will leave that question open, but we will come back to it.

Finally, as an aside, the explanation for the bear data is that bears preferentially mate with ones of their own color.  Thus, brown X white matings occur less frequently than they would in a random mating population, thus reducing the frequency of heterozygotes.








